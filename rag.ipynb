{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ecbfb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afe7f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "from html_to_markdown import convert, convert_with_inline_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45166717",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3f02d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://pokarnah.atlassian.net/wiki/api/v2/pages/65706\"\n",
    "\n",
    "auth = HTTPBasicAuth(\"pokarnah@gmail.com\", os.environ.get(\"CONFLUENCE_API_KEY\"))\n",
    "\n",
    "headers = {\n",
    "  \"Accept\": \"application/json\"\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"body-format\": \"storage\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bc45002",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.request(\n",
    "   \"GET\",\n",
    "   url,\n",
    "   headers=headers,\n",
    "   auth=auth,\n",
    "   params=params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c92cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(response.text)\n",
    "html = data['body']['storage']['value']\n",
    "dataset = convert(html=html)\n",
    "dataset = dataset.split(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4874959",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5beb90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_URL = 'https://router.huggingface.co/models/sentence-transformers/all-MiniLM-L6-v2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6a38afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b972bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    provider=\"nebius\",\n",
    "    api_key=os.environ.get('HF_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0acf6f",
   "metadata": {},
   "source": [
    "### Fixed Size Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f543648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(chunk):\n",
    "    embedding = client.feature_extraction(\n",
    "        chunk,\n",
    "        model=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    )  \n",
    "\n",
    "    return embedding[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "24e2d338",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 64\n",
    "VECTOR_DB = []\n",
    "\n",
    "for i in range(0, len(dataset), chunk_size):\n",
    "    chunk = dataset[i:i+chunk_size]\n",
    "    chunk = ' '.join(chunk)\n",
    "    embedding = create_embedding(chunk=chunk)\n",
    "    VECTOR_DB.append((chunk, embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00db312",
   "metadata": {},
   "source": [
    "# Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9269d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise Exception(\"Vectors of unequal length\")\n",
    "\n",
    "    a = np.array(a)\n",
    "    b = np.array(b)\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "\n",
    "    if norm_a == 0 or norm_b == 0:\n",
    "        raise Exception(\"Zero in denominator\")\n",
    "    similarity = dot_product / (norm_a * norm_b)\n",
    "    return similarity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d9629bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9922778767136677"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity([1, 2], [2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "77fade6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retreive(query_embedding, top_n=3):\n",
    "    similarities = []\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        query_embedding = np.array(query_embedding)\n",
    "        embedding = np.array(embedding)\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        similarities.append((chunk, similarity))\n",
    "\n",
    "    similarities.sort(key=lambda x:x[1], reverse=True)\n",
    "    return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f93d33a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input()\n",
    "query_embedding = create_embedding(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "fb6a5ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('our embeddings, documents and metadata. Importantly, collections do not require a predefined schema, we can start storing data immediately which makes ChromaDB flexible for various use cases.\\n\\n- **Documents**: Documents are the raw chunks of text we store in ChromaDB. Each document is associated with an embedding (a numerical representation of its content). We can query these documents directly making retrieval efficient and intuitive.\\n\\n## Implementation\\n\\nLets', 0.6645738947387166), ('or custom model. For example, a sentence like \"The cat is on the mat\" can be transformed into a numerical vector using a model like BERT or SentenceTransformers.\\n\\n2. **Storing Embeddings:** The embeddings are stored in a ChromaDB collection, along with optional metadata like document ID, category or timestamp and unique identifiers.\\n\\n3. **Querying:** Users can query the database by providing a vector or raw data', 0.6631374242959193), ('back their details and identifiers which can be used for search or recommendations.\\n\\n## ChromaDB Hierarchy\\n\\nIn Chroma all data about tenancy, databases, collections and documents is stored in a single SQLite database called as single-node. Below is a breakdown of this hierarchy:\\n\\nHierarchy in ChromaDB\\n\\n- **Tenants**: A tenant represents an organization or individual using ChromaDB. Each tenant logically groups together a set of databases, making it', 0.6599080285383857)]\n"
     ]
    }
   ],
   "source": [
    "retreived_knowledge = retreive(query_embedding=query_embedding)\n",
    "print(retreived_knowledge)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4909944e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
